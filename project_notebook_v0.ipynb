{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a44f577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "87362922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import requests\n",
    "from IPython.display import FileLink\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ee683",
   "metadata": {},
   "source": [
    "#### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd0f8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sncf_dataset(sncf_dataset_id, file_name, delimiter=\";\", \n",
    "                      list_separator=\",\", quote_all=\"false\", with_bom=\"true\"):\n",
    "    \"\"\"\n",
    "    Downloads a CSV from a specified SNCF dataset and returns a clickable link \n",
    "    for download in a Jupyter Notebook.\n",
    "    \n",
    "    Args:\n",
    "        id_sncf_dataset (str): The identifier of the SNCF dataset to download.\n",
    "        filename (str): Local filename to save the CSV.\n",
    "        delimiter (str, optional): Field delimiter in the CSV. Default is \";\".\n",
    "        list_separator (str, optional): Separator for list values. Default is \",\".\n",
    "        quote_all (str, optional): Whether to quote all fields. Default is \"false\".\n",
    "        with_bom (str, optional): Whether to include a BOM in the CSV. Default is \"true\".\n",
    "        \n",
    "    Returns:\n",
    "        FileLink: A clickable link to download the CSV file in the notebook.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = f\"https://data.sncf.com/api/explore/v2.1/catalog/datasets/{sncf_dataset_id}/exports/csv\"\n",
    "    params = {\n",
    "        \"delimiter\": delimiter,\n",
    "        \"list_separator\": list_separator,\n",
    "        \"quote_all\": quote_all,\n",
    "        \"with_bom\": with_bom,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    return FileLink(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17781eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='tgv-monthly-regularity.csv' target='_blank'>tgv-monthly-regularity.csv</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\kerri\\OneDrive\\Documents\\Travail\\AIDAMS\\ESSEC Y3\\Data Storage & Collection\\Project\\tgv-monthly-regularity.csv"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- SNCF Dataset Downloads ---\n",
    "\n",
    "# Regularity dataset\n",
    "dataset_id_regularity = \"regularite-mensuelle-tgv-aqst\"\n",
    "filename_regularity = \"tgv-monthly-regularity.csv\"\n",
    "sncf_dataset(dataset_id_regularity, filename_regularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5fdd3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='tgv-inoui-ouigo-fares.csv' target='_blank'>tgv-inoui-ouigo-fares.csv</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\kerri\\OneDrive\\Documents\\Travail\\AIDAMS\\ESSEC Y3\\Data Storage & Collection\\Project\\tgv-inoui-ouigo-fares.csv"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TGV/OUIGO fares dataset\n",
    "dataset_id_fares = \"tarifs-tgv-inoui-ouigo\"\n",
    "filename_fares = \"tgv-inoui-ouigo-fares.csv\"\n",
    "sncf_dataset(dataset_id_fares, filename_fares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ede2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_display_data(file_path, separator=\";\"):\n",
    "    \"\"\"\n",
    "    Loads a CSV file and displays its shape, with robust checks for errors.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        separator (str): The column separator to use.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: The specified file was not found at the path: {file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Attempt to read the file using the specified separator\n",
    "        df_raw = pd.read_csv(file_path, sep=separator)\n",
    "\n",
    "        # Check if the DataFrame is empty after loading\n",
    "        if df_raw.empty:\n",
    "            print(f\"Warning: File {file_path} was loaded but is empty.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Successful loading of file: {file_path}\")\n",
    "        print(\"---\")\n",
    "        \n",
    "        # Display the shape (number of rows, number of columns) of the DataFrame\n",
    "        print(\"DataFrame shape (rows, columns):\")\n",
    "        print(df_raw.shape)\n",
    "\n",
    "    except pd.errors.ParserError as e:\n",
    "        # Handle parsing errors (e.g., wrong separator, malformed file)\n",
    "        print(f\"Parsing Error while reading the file: {e}\")\n",
    "        print(f\"Suggestion: Check if the separator (sep='{separator}') and encoding are correct.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected error\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aefbd173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading dataset: tgv-monthly-regularity.csv ===\n",
      "Successful loading of file: tgv-monthly-regularity.csv\n",
      "---\n",
      "DataFrame shape (rows, columns):\n",
      "(10687, 26)\n",
      "✔ DataFrame 'df_reg' is ready for processing.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>service</th>\n",
       "      <th>gare_depart</th>\n",
       "      <th>gare_arrivee</th>\n",
       "      <th>duree_moyenne</th>\n",
       "      <th>nb_train_prevu</th>\n",
       "      <th>nb_annulation</th>\n",
       "      <th>commentaire_annulation</th>\n",
       "      <th>nb_train_depart_retard</th>\n",
       "      <th>retard_moyen_depart</th>\n",
       "      <th>...</th>\n",
       "      <th>nb_train_retard_sup_15</th>\n",
       "      <th>retard_moyen_trains_retard_sup15</th>\n",
       "      <th>nb_train_retard_sup_30</th>\n",
       "      <th>nb_train_retard_sup_60</th>\n",
       "      <th>prct_cause_externe</th>\n",
       "      <th>prct_cause_infra</th>\n",
       "      <th>prct_cause_gestion_trafic</th>\n",
       "      <th>prct_cause_materiel_roulant</th>\n",
       "      <th>prct_cause_gestion_gare</th>\n",
       "      <th>prct_cause_prise_en_charge_voyageurs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>National</td>\n",
       "      <td>GRENOBLE</td>\n",
       "      <td>PARIS LYON</td>\n",
       "      <td>183</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "      <td>8.027027</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>6.123741</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>17.647059</td>\n",
       "      <td>52.941176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.529412</td>\n",
       "      <td>5.882353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>International</td>\n",
       "      <td>PARIS LYON</td>\n",
       "      <td>ITALIE</td>\n",
       "      <td>394</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>11.261728</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>11.601064</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>23.809524</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>National</td>\n",
       "      <td>MARSEILLE ST CHARLES</td>\n",
       "      <td>LYON PART DIEU</td>\n",
       "      <td>106</td>\n",
       "      <td>557</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133</td>\n",
       "      <td>6.978195</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>5.195333</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>19.230769</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>3.846154</td>\n",
       "      <td>7.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>National</td>\n",
       "      <td>PARIS NORD</td>\n",
       "      <td>DUNKERQUE</td>\n",
       "      <td>116</td>\n",
       "      <td>271</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>11.236594</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>3.738806</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>National</td>\n",
       "      <td>ANNECY</td>\n",
       "      <td>PARIS LYON</td>\n",
       "      <td>224</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>8.070833</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>8.552525</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>23.809524</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>4.761905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date        service           gare_depart    gare_arrivee  \\\n",
       "0  2018-01       National              GRENOBLE      PARIS LYON   \n",
       "1  2018-01  International            PARIS LYON          ITALIE   \n",
       "2  2018-01       National  MARSEILLE ST CHARLES  LYON PART DIEU   \n",
       "3  2018-01       National            PARIS NORD       DUNKERQUE   \n",
       "4  2018-01       National                ANNECY      PARIS LYON   \n",
       "\n",
       "   duree_moyenne  nb_train_prevu  nb_annulation  commentaire_annulation  \\\n",
       "0            183             245              0                     NaN   \n",
       "1            394              94              0                     NaN   \n",
       "2            106             557              7                     NaN   \n",
       "3            116             271              3                     NaN   \n",
       "4            224             198              0                     NaN   \n",
       "\n",
       "   nb_train_depart_retard  retard_moyen_depart  ...  nb_train_retard_sup_15  \\\n",
       "0                      37             8.027027  ...                      25   \n",
       "1                      27            11.261728  ...                      22   \n",
       "2                     133             6.978195  ...                      40   \n",
       "3                      46            11.236594  ...                      18   \n",
       "4                      12             8.070833  ...                      38   \n",
       "\n",
       "   retard_moyen_trains_retard_sup15  nb_train_retard_sup_30  \\\n",
       "0                          6.123741                      13   \n",
       "1                         11.601064                      15   \n",
       "2                          5.195333                      19   \n",
       "3                          3.738806                       9   \n",
       "4                          8.552525                      14   \n",
       "\n",
       "   nb_train_retard_sup_60  prct_cause_externe prct_cause_infra  \\\n",
       "0                       6           17.647059        52.941176   \n",
       "1                       6           33.333333        19.047619   \n",
       "2                       5           23.076923        23.076923   \n",
       "3                       4           35.714286        28.571429   \n",
       "4                       5           23.809524        42.857143   \n",
       "\n",
       "   prct_cause_gestion_trafic  prct_cause_materiel_roulant  \\\n",
       "0                   0.000000                    23.529412   \n",
       "1                  23.809524                    14.285714   \n",
       "2                  19.230769                    23.076923   \n",
       "3                   7.142857                    25.000000   \n",
       "4                   9.523810                    14.285714   \n",
       "\n",
       "   prct_cause_gestion_gare  prct_cause_prise_en_charge_voyageurs  \n",
       "0                 5.882353                              0.000000  \n",
       "1                 9.523810                              0.000000  \n",
       "2                 3.846154                              7.692308  \n",
       "3                 3.571429                              0.000000  \n",
       "4                 4.761905                              4.761905  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Load df_reg ====\n",
    "print(\"\\n=== Loading dataset: tgv-monthly-regularity.csv ===\")\n",
    "file_name = \"tgv-monthly-regularity.csv\"\n",
    "df_reg = load_and_display_data(file_name, separator=\";\")\n",
    "\n",
    "if df_reg is not None:\n",
    "    print(\"✔ DataFrame 'df_reg' is ready for processing.\\n\")\n",
    "    display(df_reg.head())\n",
    "else:\n",
    "    print(\"✘ Failed to load 'df_reg'. Processing stopped.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c618755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading dataset: tgv-inoui-ouigo-fares.csv ===\n",
      "Successful loading of file: tgv-inoui-ouigo-fares.csv\n",
      "---\n",
      "DataFrame shape (rows, columns):\n",
      "(34135, 9)\n",
      "✔ DataFrame 'df_far' is ready for processing.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transporteur</th>\n",
       "      <th>gare_origine</th>\n",
       "      <th>gare_origine_code_uic</th>\n",
       "      <th>gare_destination</th>\n",
       "      <th>gare_destination_code_uic</th>\n",
       "      <th>classe</th>\n",
       "      <th>profil_tarifaire</th>\n",
       "      <th>prix_minimum</th>\n",
       "      <th>prix_maximum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>AVIGNON TGV</td>\n",
       "      <td>87318964</td>\n",
       "      <td>PERPIGNAN</td>\n",
       "      <td>87784009</td>\n",
       "      <td>1</td>\n",
       "      <td>Tarif Réglementé</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>CHAMBERY CHALLES LES EAUX</td>\n",
       "      <td>87741009</td>\n",
       "      <td>LEPIN LE LAC LA BAUCHE</td>\n",
       "      <td>87741439</td>\n",
       "      <td>1</td>\n",
       "      <td>Tarif Réglementé</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>BAR LE DUC</td>\n",
       "      <td>87175042</td>\n",
       "      <td>CHALONS EN CHAMPAGNE</td>\n",
       "      <td>87174003</td>\n",
       "      <td>1</td>\n",
       "      <td>Tarif Réglementé</td>\n",
       "      <td>28.4</td>\n",
       "      <td>28.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>BREST</td>\n",
       "      <td>87474007</td>\n",
       "      <td>NANTES</td>\n",
       "      <td>87481002</td>\n",
       "      <td>2</td>\n",
       "      <td>Tarif Réglementé</td>\n",
       "      <td>60.9</td>\n",
       "      <td>60.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>BAYONNE</td>\n",
       "      <td>87673004</td>\n",
       "      <td>CHATELLERAULT</td>\n",
       "      <td>87575142</td>\n",
       "      <td>1</td>\n",
       "      <td>Tarif Elève - Etudiant - Apprenti</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transporteur               gare_origine  gare_origine_code_uic  \\\n",
       "0    TGV INOUI                AVIGNON TGV               87318964   \n",
       "1    TGV INOUI  CHAMBERY CHALLES LES EAUX               87741009   \n",
       "2    TGV INOUI                 BAR LE DUC               87175042   \n",
       "3    TGV INOUI                      BREST               87474007   \n",
       "4    TGV INOUI                    BAYONNE               87673004   \n",
       "\n",
       "         gare_destination  gare_destination_code_uic  classe  \\\n",
       "0               PERPIGNAN                   87784009       1   \n",
       "1  LEPIN LE LAC LA BAUCHE                   87741439       1   \n",
       "2    CHALONS EN CHAMPAGNE                   87174003       1   \n",
       "3                  NANTES                   87481002       2   \n",
       "4           CHATELLERAULT                   87575142       1   \n",
       "\n",
       "                    profil_tarifaire  prix_minimum  prix_maximum  \n",
       "0                   Tarif Réglementé          67.0          67.0  \n",
       "1                   Tarif Réglementé           7.1           7.1  \n",
       "2                   Tarif Réglementé          28.4          28.4  \n",
       "3                   Tarif Réglementé          60.9          60.9  \n",
       "4  Tarif Elève - Etudiant - Apprenti          15.0          15.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Load df_far ====\n",
    "print(\"\\n=== Loading dataset: tgv-inoui-ouigo-fares.csv ===\")\n",
    "file_name = \"tgv-inoui-ouigo-fares.csv\"\n",
    "df_far = load_and_display_data(file_name, separator=\";\")\n",
    "\n",
    "if df_far is not None:\n",
    "    print(\"✔ DataFrame 'df_far' is ready for processing.\\n\")\n",
    "    display(df_far.head())\n",
    "else:\n",
    "    print(\"✘ Failed to load 'df_far'. Processing stopped.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2db05826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataframe_columns(df, df_name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Print all column names of a DataFrame with a clean, numbered format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame whose columns should be displayed.\n",
    "    df_name : str, optional\n",
    "        A label used for printing, by default \"DataFrame\".\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n## Columns in {df_name}\\n\")\n",
    "    \n",
    "    print(\"Column names:\")\n",
    "    for i, col in enumerate(df.columns.tolist(), 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "\n",
    "    print(f\"\\nTotal Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a507abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Columns in df_reg\n",
      "\n",
      "Column names:\n",
      "  1. date\n",
      "  2. service\n",
      "  3. gare_depart\n",
      "  4. gare_arrivee\n",
      "  5. duree_moyenne\n",
      "  6. nb_train_prevu\n",
      "  7. nb_annulation\n",
      "  8. commentaire_annulation\n",
      "  9. nb_train_depart_retard\n",
      "  10. retard_moyen_depart\n",
      "  11. retard_moyen_tous_trains_depart\n",
      "  12. commentaire_retards_depart\n",
      "  13. nb_train_retard_arrivee\n",
      "  14. retard_moyen_arrivee\n",
      "  15. retard_moyen_tous_trains_arrivee\n",
      "  16. commentaires_retard_arrivee\n",
      "  17. nb_train_retard_sup_15\n",
      "  18. retard_moyen_trains_retard_sup15\n",
      "  19. nb_train_retard_sup_30\n",
      "  20. nb_train_retard_sup_60\n",
      "  21. prct_cause_externe\n",
      "  22. prct_cause_infra\n",
      "  23. prct_cause_gestion_trafic\n",
      "  24. prct_cause_materiel_roulant\n",
      "  25. prct_cause_gestion_gare\n",
      "  26. prct_cause_prise_en_charge_voyageurs\n",
      "\n",
      "Total Columns: 26\n"
     ]
    }
   ],
   "source": [
    "# Print columns of df_reg\n",
    "print_dataframe_columns(df_reg, df_name=\"df_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d7e51a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Columns in df_far\n",
      "\n",
      "Column names:\n",
      "  1. transporteur\n",
      "  2. gare_origine\n",
      "  3. gare_origine_code_uic\n",
      "  4. gare_destination\n",
      "  5. gare_destination_code_uic\n",
      "  6. classe\n",
      "  7. profil_tarifaire\n",
      "  8. prix_minimum\n",
      "  9. prix_maximum\n",
      "\n",
      "Total Columns: 9\n"
     ]
    }
   ],
   "source": [
    "# Print columns of df_far\n",
    "print_dataframe_columns(df_far, df_name=\"df_far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "070eef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dataframe_columns(df, rename_dict):\n",
    "    \"\"\"\n",
    "    Rename the columns of a DataFrame based on a dictionary mapping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame whose columns should be renamed.\n",
    "    rename_dict : dict\n",
    "        Dictionary mapping old column names to new column names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "044fdccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>service</th>\n",
       "      <th>departure_station</th>\n",
       "      <th>arrival_station</th>\n",
       "      <th>avg_trip_duration</th>\n",
       "      <th>scheduled_trains</th>\n",
       "      <th>canceled_trains</th>\n",
       "      <th>cancellation_comments</th>\n",
       "      <th>trains_delayed_departure</th>\n",
       "      <th>avg_delay_delayed_trains_departure</th>\n",
       "      <th>...</th>\n",
       "      <th>trains_delayed_over_15min</th>\n",
       "      <th>avg_delay_over_15min</th>\n",
       "      <th>trains_delayed_over_30min</th>\n",
       "      <th>trains_delayed_over_60min</th>\n",
       "      <th>pct_delay_external_causes</th>\n",
       "      <th>pct_delay_infrastructure</th>\n",
       "      <th>pct_delay_traffic_management</th>\n",
       "      <th>pct_delay_rolling_stock</th>\n",
       "      <th>pct_delay_station_operations</th>\n",
       "      <th>pct_delay_passenger_handling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>National</td>\n",
       "      <td>GRENOBLE</td>\n",
       "      <td>PARIS LYON</td>\n",
       "      <td>183</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "      <td>8.027027</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>6.123741</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>17.647059</td>\n",
       "      <td>52.941176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.529412</td>\n",
       "      <td>5.882353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>International</td>\n",
       "      <td>PARIS LYON</td>\n",
       "      <td>ITALIE</td>\n",
       "      <td>394</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>11.261728</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>11.601064</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>23.809524</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>National</td>\n",
       "      <td>MARSEILLE ST CHARLES</td>\n",
       "      <td>LYON PART DIEU</td>\n",
       "      <td>106</td>\n",
       "      <td>557</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133</td>\n",
       "      <td>6.978195</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>5.195333</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>19.230769</td>\n",
       "      <td>23.076923</td>\n",
       "      <td>3.846154</td>\n",
       "      <td>7.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>National</td>\n",
       "      <td>PARIS NORD</td>\n",
       "      <td>DUNKERQUE</td>\n",
       "      <td>116</td>\n",
       "      <td>271</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>11.236594</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>3.738806</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>National</td>\n",
       "      <td>ANNECY</td>\n",
       "      <td>PARIS LYON</td>\n",
       "      <td>224</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>8.070833</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>8.552525</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>23.809524</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>4.761905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date        service     departure_station arrival_station  \\\n",
       "0  2018-01       National              GRENOBLE      PARIS LYON   \n",
       "1  2018-01  International            PARIS LYON          ITALIE   \n",
       "2  2018-01       National  MARSEILLE ST CHARLES  LYON PART DIEU   \n",
       "3  2018-01       National            PARIS NORD       DUNKERQUE   \n",
       "4  2018-01       National                ANNECY      PARIS LYON   \n",
       "\n",
       "   avg_trip_duration  scheduled_trains  canceled_trains  \\\n",
       "0                183               245                0   \n",
       "1                394                94                0   \n",
       "2                106               557                7   \n",
       "3                116               271                3   \n",
       "4                224               198                0   \n",
       "\n",
       "   cancellation_comments  trains_delayed_departure  \\\n",
       "0                    NaN                        37   \n",
       "1                    NaN                        27   \n",
       "2                    NaN                       133   \n",
       "3                    NaN                        46   \n",
       "4                    NaN                        12   \n",
       "\n",
       "   avg_delay_delayed_trains_departure  ...  trains_delayed_over_15min  \\\n",
       "0                            8.027027  ...                         25   \n",
       "1                           11.261728  ...                         22   \n",
       "2                            6.978195  ...                         40   \n",
       "3                           11.236594  ...                         18   \n",
       "4                            8.070833  ...                         38   \n",
       "\n",
       "   avg_delay_over_15min  trains_delayed_over_30min  trains_delayed_over_60min  \\\n",
       "0              6.123741                         13                          6   \n",
       "1             11.601064                         15                          6   \n",
       "2              5.195333                         19                          5   \n",
       "3              3.738806                          9                          4   \n",
       "4              8.552525                         14                          5   \n",
       "\n",
       "   pct_delay_external_causes pct_delay_infrastructure  \\\n",
       "0                  17.647059                52.941176   \n",
       "1                  33.333333                19.047619   \n",
       "2                  23.076923                23.076923   \n",
       "3                  35.714286                28.571429   \n",
       "4                  23.809524                42.857143   \n",
       "\n",
       "   pct_delay_traffic_management  pct_delay_rolling_stock  \\\n",
       "0                      0.000000                23.529412   \n",
       "1                     23.809524                14.285714   \n",
       "2                     19.230769                23.076923   \n",
       "3                      7.142857                25.000000   \n",
       "4                      9.523810                14.285714   \n",
       "\n",
       "   pct_delay_station_operations  pct_delay_passenger_handling  \n",
       "0                      5.882353                      0.000000  \n",
       "1                      9.523810                      0.000000  \n",
       "2                      3.846154                      7.692308  \n",
       "3                      3.571429                      0.000000  \n",
       "4                      4.761905                      4.761905  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rename_dict_reg = {\n",
    "    \"date\": \"date\",\n",
    "    \"service\": \"service\",\n",
    "    \"gare_depart\": \"departure_station\",\n",
    "    \"gare_arrivee\": \"arrival_station\",\n",
    "    \"duree_moyenne\": \"avg_trip_duration\",\n",
    "    \"nb_train_prevu\": \"scheduled_trains\",\n",
    "    \"nb_annulation\": \"canceled_trains\",\n",
    "    \"commentaire_annulation\": \"cancellation_comments\",\n",
    "    \"nb_train_depart_retard\": \"trains_delayed_departure\",\n",
    "    \"retard_moyen_depart\": \"avg_delay_delayed_trains_departure\",\n",
    "    \"retard_moyen_tous_trains_depart\": \"avg_delay_all_trains_departure\",\n",
    "    \"commentaire_retards_depart\": \"departure_delay_comments\",\n",
    "    \"nb_train_retard_arrivee\": \"trains_delayed_arrival\",\n",
    "    \"retard_moyen_arrivee\": \"avg_delay_delayed_trains_arrival\",\n",
    "    \"retard_moyen_tous_trains_arrivee\": \"avg_delay_all_trains_arrival\",\n",
    "    \"commentaires_retard_arrivee\": \"arrival_delay_comments\",\n",
    "    \"nb_train_retard_sup_15\": \"trains_delayed_over_15min\",\n",
    "    \"retard_moyen_trains_retard_sup15\": \"avg_delay_over_15min\",\n",
    "    \"nb_train_retard_sup_30\": \"trains_delayed_over_30min\",\n",
    "    \"nb_train_retard_sup_60\": \"trains_delayed_over_60min\",\n",
    "    \"prct_cause_externe\": \"pct_delay_external_causes\",\n",
    "    \"prct_cause_infra\": \"pct_delay_infrastructure\",\n",
    "    \"prct_cause_gestion_trafic\": \"pct_delay_traffic_management\",\n",
    "    \"prct_cause_materiel_roulant\": \"pct_delay_rolling_stock\",\n",
    "    \"prct_cause_gestion_gare\": \"pct_delay_station_operations\",\n",
    "    \"prct_cause_prise_en_charge_voyageurs\": \"pct_delay_passenger_handling\"\n",
    "}\n",
    "\n",
    "df_reg = rename_dataframe_columns(df_reg, rename_dict_reg)\n",
    "display(df_reg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "087c995c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tgv_types</th>\n",
       "      <th>departure_station</th>\n",
       "      <th>departure_station_uic</th>\n",
       "      <th>arrival_station</th>\n",
       "      <th>arrival_station_uic</th>\n",
       "      <th>classe</th>\n",
       "      <th>fare_profile</th>\n",
       "      <th>min_price</th>\n",
       "      <th>max_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>AVIGNON TGV</td>\n",
       "      <td>87318964</td>\n",
       "      <td>PERPIGNAN</td>\n",
       "      <td>87784009</td>\n",
       "      <td>1</td>\n",
       "      <td>Tarif Réglementé</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>CHAMBERY CHALLES LES EAUX</td>\n",
       "      <td>87741009</td>\n",
       "      <td>LEPIN LE LAC LA BAUCHE</td>\n",
       "      <td>87741439</td>\n",
       "      <td>1</td>\n",
       "      <td>Tarif Réglementé</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>BAR LE DUC</td>\n",
       "      <td>87175042</td>\n",
       "      <td>CHALONS EN CHAMPAGNE</td>\n",
       "      <td>87174003</td>\n",
       "      <td>1</td>\n",
       "      <td>Tarif Réglementé</td>\n",
       "      <td>28.4</td>\n",
       "      <td>28.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>BREST</td>\n",
       "      <td>87474007</td>\n",
       "      <td>NANTES</td>\n",
       "      <td>87481002</td>\n",
       "      <td>2</td>\n",
       "      <td>Tarif Réglementé</td>\n",
       "      <td>60.9</td>\n",
       "      <td>60.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TGV INOUI</td>\n",
       "      <td>BAYONNE</td>\n",
       "      <td>87673004</td>\n",
       "      <td>CHATELLERAULT</td>\n",
       "      <td>87575142</td>\n",
       "      <td>1</td>\n",
       "      <td>Tarif Elève - Etudiant - Apprenti</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tgv_types          departure_station  departure_station_uic  \\\n",
       "0  TGV INOUI                AVIGNON TGV               87318964   \n",
       "1  TGV INOUI  CHAMBERY CHALLES LES EAUX               87741009   \n",
       "2  TGV INOUI                 BAR LE DUC               87175042   \n",
       "3  TGV INOUI                      BREST               87474007   \n",
       "4  TGV INOUI                    BAYONNE               87673004   \n",
       "\n",
       "          arrival_station  arrival_station_uic  classe  \\\n",
       "0               PERPIGNAN             87784009       1   \n",
       "1  LEPIN LE LAC LA BAUCHE             87741439       1   \n",
       "2    CHALONS EN CHAMPAGNE             87174003       1   \n",
       "3                  NANTES             87481002       2   \n",
       "4           CHATELLERAULT             87575142       1   \n",
       "\n",
       "                        fare_profile  min_price  max_price  \n",
       "0                   Tarif Réglementé       67.0       67.0  \n",
       "1                   Tarif Réglementé        7.1        7.1  \n",
       "2                   Tarif Réglementé       28.4       28.4  \n",
       "3                   Tarif Réglementé       60.9       60.9  \n",
       "4  Tarif Elève - Etudiant - Apprenti       15.0       15.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rename_dict_far = {\n",
    "    \"transporteur\": \"tgv_types\",\n",
    "    \"gare_origine\": \"departure_station\",\n",
    "    \"gare_origine_code_uic\": \"departure_station_uic\",\n",
    "    \"gare_destination\": \"arrival_station\",\n",
    "    \"gare_destination_code_uic\": \"arrival_station_uic\",\n",
    "    \"classe\t\": \"train_class\",\n",
    "    \"profil_tarifaire\": \"fare_profile\",\n",
    "    \"prix_minimum\": \"min_price\",\n",
    "    \"prix_maximum\": \"max_price\"\n",
    "}\n",
    "\n",
    "\n",
    "df_far = rename_dataframe_columns(df_far, rename_dict_far)\n",
    "display(df_far.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93b2b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "## DataFrame Information: df_reg\n",
      "============================================================\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10687 entries, 0 to 10686\n",
      "Data columns (total 26 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   date                                10687 non-null  object \n",
      " 1   service                             10687 non-null  object \n",
      " 2   departure_station                   10687 non-null  object \n",
      " 3   arrival_station                     10687 non-null  object \n",
      " 4   avg_trip_duration                   10687 non-null  int64  \n",
      " 5   scheduled_trains                    10687 non-null  int64  \n",
      " 6   canceled_trains                     10687 non-null  int64  \n",
      " 7   cancellation_comments               0 non-null      float64\n",
      " 8   trains_delayed_departure            10687 non-null  int64  \n",
      " 9   avg_delay_delayed_trains_departure  10687 non-null  float64\n",
      " 10  avg_delay_all_trains_departure      10687 non-null  float64\n",
      " 11  departure_delay_comments            0 non-null      float64\n",
      " 12  trains_delayed_arrival              10687 non-null  int64  \n",
      " 13  avg_delay_delayed_trains_arrival    10687 non-null  float64\n",
      " 14  avg_delay_all_trains_arrival        10687 non-null  float64\n",
      " 15  arrival_delay_comments              698 non-null    object \n",
      " 16  trains_delayed_over_15min           10687 non-null  int64  \n",
      " 17  avg_delay_over_15min                10687 non-null  float64\n",
      " 18  trains_delayed_over_30min           10687 non-null  int64  \n",
      " 19  trains_delayed_over_60min           10687 non-null  int64  \n",
      " 20  pct_delay_external_causes           10687 non-null  float64\n",
      " 21  pct_delay_infrastructure            10687 non-null  float64\n",
      " 22  pct_delay_traffic_management        10687 non-null  float64\n",
      " 23  pct_delay_rolling_stock             10687 non-null  float64\n",
      " 24  pct_delay_station_operations        10687 non-null  float64\n",
      " 25  pct_delay_passenger_handling        10687 non-null  float64\n",
      "dtypes: float64(13), int64(8), object(5)\n",
      "memory usage: 5.0 MB\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== DataFrame Summary: df_reg =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"## DataFrame Information: df_reg\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "df_reg.info(memory_usage='deep')\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1b763b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "## DataFrame Information: df_far\n",
      "============================================================\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34135 entries, 0 to 34134\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   tgv_types              34135 non-null  object \n",
      " 1   departure_station      34135 non-null  object \n",
      " 2   departure_station_uic  34135 non-null  int64  \n",
      " 3   arrival_station        34135 non-null  object \n",
      " 4   arrival_station_uic    34135 non-null  int64  \n",
      " 5   classe                 34135 non-null  int64  \n",
      " 6   fare_profile           34135 non-null  object \n",
      " 7   min_price              34135 non-null  float64\n",
      " 8   max_price              34135 non-null  float64\n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 9.7 MB\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== DataFrame Summary: df_far =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"## DataFrame Information: df_far\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "df_far.info(memory_usage='deep')\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5acc50",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c94bf45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_profiling_report(df, output_file=\"profiling_report.html\", title=\"Data Profiling Report\"):\n",
    "    \"\"\"\n",
    "    Generate an HTML profiling report for a DataFrame using ydata_profiling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to profile.\n",
    "    output_file : str, optional\n",
    "        The name of the output HTML file (default is \"profiling_report.html\").\n",
    "    title : str, optional\n",
    "        The title of the profiling report (default is \"Data Profiling Report\").\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n## Generating Profiling Report: {title} ...\")\n",
    "        \n",
    "        profile = ProfileReport(\n",
    "            df,\n",
    "            title=title,\n",
    "            sort=None,        # Use 'None' for original order, 'alphabetical' for sorting\n",
    "            explorative=True\n",
    "        )\n",
    "        \n",
    "        profile.to_file(output_file)\n",
    "        print(f\"✔ Profiling report successfully exported to: {output_file}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Error: The 'ydata-profiling' library is required.\")\n",
    "        print(\"Install it using: `pip install ydata-profiling`\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during report generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d68b7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Generating Profiling Report: TGV Regularity Profiling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 306.03it/s]<00:00, 39.27it/s, Describe variable: pct_delay_passenger_handling]\n",
      "Summarize dataset: 100%|██████████| 396/396 [00:27<00:00, 14.65it/s, Completed]                                                                     \n",
      "Generate report structure: 100%|██████████| 1/1 [00:04<00:00,  4.83s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Profiling report successfully exported to: tgv_regularity_report.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate profiling report for df_reg\n",
    "generate_profiling_report(df_reg, output_file=\"tgv_regularity_report.html\", title=\"TGV Regularity Profiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4eab9293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Generating Profiling Report: TGV Fares Profiling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 319.75it/s]0<00:01,  5.63it/s, Describe variable: max_price]\n",
      "Summarize dataset: 100%|██████████| 34/34 [00:03<00:00, 11.23it/s, Completed]                                           \n",
      "Generate report structure: 100%|██████████| 1/1 [00:05<00:00,  5.52s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 58.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Profiling report successfully exported to: tgv_fares_report.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate profiling report for df_far\n",
    "generate_profiling_report(df_far, output_file=\"tgv_fares_report.html\", title=\"TGV Fares Profiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c4825e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_summary(df, df_name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Calculate and display missing values (count and percentage) for a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to analyze.\n",
    "    df_name : str, optional\n",
    "        Name of the DataFrame for display purposes (default is \"DataFrame\").\n",
    "    \"\"\"\n",
    "    missing = pd.DataFrame({\n",
    "        'Missing Count': df.isna().sum(),\n",
    "        'Missing Percent': (df.isna().sum() / len(df) * 100).round(2)\n",
    "    }).sort_values('Missing Percent', ascending=False)\n",
    "    \n",
    "    print(f\"\\n## Missing Value Summary for {df_name}\\n\")\n",
    "    \n",
    "    # Filter for columns with at least one missing value\n",
    "    missing_data_summary = missing[missing['Missing Count'] > 0]\n",
    "    \n",
    "    if not missing_data_summary.empty:\n",
    "        print(f\"{len(missing_data_summary)} columns have missing values.\")\n",
    "        print(\"---\")\n",
    "        display(missing_data_summary)\n",
    "    else:\n",
    "        print(f\"Great! No missing values found in {df_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e207326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Missing Value Summary for df_reg\n",
      "\n",
      "3 columns have missing values.\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cancellation_comments</th>\n",
       "      <td>10687</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>departure_delay_comments</th>\n",
       "      <td>10687</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrival_delay_comments</th>\n",
       "      <td>9989</td>\n",
       "      <td>93.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Missing Count  Missing Percent\n",
       "cancellation_comments             10687           100.00\n",
       "departure_delay_comments          10687           100.00\n",
       "arrival_delay_comments             9989            93.47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate missing values summary for df_far\n",
    "missing_values_summary(df_reg, \"df_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "58337495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Missing Value Summary for df_far\n",
      "\n",
      "Great! No missing values found in df_far.\n"
     ]
    }
   ],
   "source": [
    "# Generate missing values summary for df_far\n",
    "missing_values_summary(df_far, \"df_far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02140d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Column Deletion Summary\n",
      "\n",
      "Successfully dropped the following columns (if they existed):\n",
      "- Commentaire annulations\n",
      "- Commentaire retards au départ\n",
      "\n",
      "---\n",
      "New DataFrame shape: (10687, 26)\n"
     ]
    }
   ],
   "source": [
    "# Define the list of columns to be dropped\n",
    "# These columns often contain qualitative/text data or a high percentage of missing values.\n",
    "columns_to_drop = [\n",
    "    'Commentaire annulations',\n",
    "    'Commentaire retards au départ'\n",
    "]\n",
    "\n",
    "# Use a single efficient .drop() call\n",
    "# The 'errors=\"ignore\"' argument is a key improvement: it prevents the code from\n",
    "# crashing if one of the columns listed above was already dropped or doesn't exist.\n",
    "df_raw = df_raw.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "print(\"\\n## Column Deletion Summary\\n\")\n",
    "print(f\"Successfully dropped the following columns (if they existed):\")\n",
    "\n",
    "for col in columns_to_drop:\n",
    "    print(f\"- {col}\")\n",
    "        \n",
    "print(\"\\n---\")\n",
    "print(f\"New DataFrame shape: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7bc2c917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Duplicate Row Check\n",
      "\n",
      "Great! No full-row duplicates were found in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of duplicate rows (where the entire row is identical)\n",
    "num_duplicates = df_raw.duplicated().sum()\n",
    "    \n",
    "print(\"\\n## Duplicate Row Check\\n\")\n",
    "    \n",
    "if num_duplicates > 0:\n",
    "    print(f\"Warning: Found {num_duplicates} duplicate row(s) in the DataFrame.\")\n",
    "        \n",
    "    # Optional: Print the percentage of duplicate rows for better context\n",
    "    percent_duplicates = (num_duplicates / len(df_raw) * 100).round(2)\n",
    "    print(f\"This represents {percent_duplicates}% of the total data.\")\n",
    "        \n",
    "    # Optional: Display the first few duplicate rows for inspection\n",
    "    # Keep='first' means the second, third, etc., occurrences are marked True\n",
    "    print(\"\\nFirst 5 duplicate entries (excluding the first occurrence):\")\n",
    "    display(df_raw[df_raw.duplicated(keep='first')].head())\n",
    "        \n",
    "else:\n",
    "    print(\"Great! No full-row duplicates were found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29992328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Trimming Whitespace in String Columns\n",
      "\n",
      "Whitespace trimmed in 5 object columns.\n",
      "Columns processed:\n",
      "- date\n",
      "- service\n",
      "- gare_depart\n",
      "- gare_arrivee\n",
      "- commentaires_retard_arrivee\n"
     ]
    }
   ],
   "source": [
    "# Identify object (string) columns using select_dtypes\n",
    "# This is often more explicit and faster than iterating over all columns and checking dtype\n",
    "object_cols = df_raw.select_dtypes(include=['object']).columns\n",
    "    \n",
    "print(\"\\n## Trimming Whitespace in String Columns\\n\")\n",
    "    \n",
    "if len(object_cols) > 0:\n",
    "        \n",
    "# Apply the strip function using .loc for explicit assignment and better performance\n",
    "    for col in object_cols:\n",
    "        # We explicitly handle potential NaN values by converting them to string\n",
    "        # and then applying str.strip(). This ensures all cells are processed, \n",
    "        # though str.strip() on a true NaN string ('nan') will result in 'nan'.\n",
    "        # A more robust approach, applied here, is to use .str.strip() which \n",
    "        # correctly handles NaN values (keeping them as NaN).\n",
    "            \n",
    "        # The .str accessor safely applies the string method, returning NaN for NaN inputs.\n",
    "        df_raw.loc[:, col] = df_raw[col].str.strip() \n",
    "\n",
    "    print(f\"Whitespace trimmed in {len(object_cols)} object columns.\")\n",
    "    print(\"Columns processed:\")\n",
    "\n",
    "    for col in object_cols:\n",
    "        print(f\"- {col}\")\n",
    "else:\n",
    "    print(\"Warning: No columns of type 'object' found to trim whitespace.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6440586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Date Column Conversion (Date)\n",
      "\n",
      "Warning: Column 'Date' not found in the DataFrame. Conversion skipped.\n"
     ]
    }
   ],
   "source": [
    "date_column = \"Date\"\n",
    "    \n",
    "print(f\"\\n## Date Column Conversion ({date_column})\\n\")\n",
    "\n",
    "# Check if the 'Date' column exists\n",
    "if date_column in df_raw.columns:\n",
    "        \n",
    "    # Store the original dtype for comparison\n",
    "    original_dtype = df_raw[date_column].dtype\n",
    "        \n",
    "    try:\n",
    "        # Use errors='coerce' to handle values that don't match the specified format.\n",
    "        # These values will be converted to NaT (Not a Time), which is preferable \n",
    "        # to crashing the script.\n",
    "        df_raw[date_column] = pd.to_datetime(\n",
    "            df_raw[date_column], \n",
    "            format=\"%Y-%m\",\n",
    "            errors='coerce' # Key improvement for error handling\n",
    "        )\n",
    "\n",
    "        new_dtype = df_raw[date_column].dtype\n",
    "        print(f\"Conversion successful!\")\n",
    "        print(f\"Original data type: {original_dtype}\")\n",
    "        print(f\"New data type: {new_dtype}\")\n",
    "\n",
    "        # Check for NaT values introduced during coercion\n",
    "        nat_count = df_raw[date_column].isna().sum()\n",
    "        if nat_count > 0:\n",
    "            print(f\"Warning: {nat_count} value(s) could not be parsed and were converted to NaT.\")\n",
    "                \n",
    "        print(\"\\nFirst 5 rows with new date format:\")\n",
    "        display(df_raw.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during date conversion: {e}\")\n",
    "        print(\"Suggestion: Verify the data format and column content.\")\n",
    "            \n",
    "else:\n",
    "    print(f\"Warning: Column '{date_column}' not found in the DataFrame. Conversion skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4736e9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Nombre de circulations prévues'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kerri\\anaconda3\\envs\\data_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Nombre de circulations prévues'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# checking that 'Nombre de circulations prévues' >= 'Nombre de trains annulés'\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m invalid_rows = df_raw[\u001b[43mdf_raw\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNombre de circulations prévues\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m < df_raw[\u001b[33m'\u001b[39m\u001b[33mNombre de trains annulés\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of rows where the number of scheduled trains is less than the number of cancelled trains: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(invalid_rows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRows where the number of trains scheduled trains equals the number of cancelled trains:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kerri\\anaconda3\\envs\\data_env\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kerri\\anaconda3\\envs\\data_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Nombre de circulations prévues'"
     ]
    }
   ],
   "source": [
    "# checking that 'Nombre de circulations prévues' >= 'Nombre de trains annulés'\n",
    "\n",
    "invalid_rows = df_raw[df_raw['Nombre de circulations prévues'] < df_raw['Nombre de trains annulés']]\n",
    "print(f\"Number of rows where the number of scheduled trains is less than the number of cancelled trains: {len(invalid_rows)}\")\n",
    "\n",
    "print(\"Rows where the number of trains scheduled trains equals the number of cancelled trains:\")\n",
    "df_raw[df_raw['Nombre de circulations prévues'] == df_raw['Nombre de trains annulés']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f551ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute rows where 'Nombre de circulations prévues' < 'Nombre de trains annulés' with the mean of 'Nombre de circulations prévues' for trains with same 'Gare de départ' and 'Gare d'arrivée' \n",
    "for index, row in invalid_rows.iterrows():\n",
    "    mask = (df_raw['Gare de départ'] == row['Gare de départ']) & (df_raw['Gare d\\'arrivée'] == row['Gare d\\'arrivée']) & (df_raw.index != index)\n",
    "    mean_value = df_raw.loc[mask, 'Nombre de circulations prévues'].mean()\n",
    "    df_raw.at[index, 'Nombre de circulations prévues'] = mean_value\n",
    "\n",
    "# Verify that there are no more invalid rows\n",
    "invalid_rows_after = df_raw[df_raw['Nombre de circulations prévues'] < df_raw['Nombre de trains annulés']]\n",
    "print(f\"Number of invalid rows after imputation: {len(invalid_rows_after)}\")\n",
    "\n",
    "invalid_rows_after.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33175361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking why there are still 10 rows where nb of scheduled trains is inferior to nb of cancelled trains\n",
    "\n",
    "# check trains with 'Gare de départ' NANTES and 'Gare d'arrivée' STRASBOURG\n",
    "df_raw[(df_raw['Gare de départ'] == 'NANTES') & (df_raw['Gare d\\'arrivée'] == 'STRASBOURG')].head()\n",
    "\n",
    "# check trains with 'Gare de départ' MARSEILLE ST CHARLES and 'Gare d'arrivée' TOURCOING\n",
    "df_raw[(df_raw['Gare de départ'] == 'MARSEILLE ST CHARLES') & (df_raw['Gare d\\'arrivée'] == 'TOURCOING')].head()\n",
    "\n",
    "# check trains with 'Gare de départ' BORDEAUX ST JEAN and 'Gare d'arrivée' TOURCOING\t\n",
    "df_raw[(df_raw['Gare de départ'] == 'BORDEAUX ST JEAN') & (df_raw['Gare d\\'arrivée'] == 'BORDEAUX ST JEAN')].head()\n",
    "\n",
    "# check trains with 'Gare de départ' TOURCOING and 'Gare d'arrivée' BORDEAUX ST JEAN\t\n",
    "df_raw[(df_raw['Gare de départ'] == 'TOURCOING') & (df_raw['Gare d\\'arrivée'] == 'BORDEAUX ST JEAN')].head()\n",
    "\n",
    "# check trains with 'Gare de départ' MADRID and 'Gare d'arrivée' MARSEILLE ST CHARLES\n",
    "df_raw[(df_raw['Gare de départ'] == 'MADRID') & (df_raw['Gare d\\'arrivée'] == 'MARSEILLE ST CHARLES')].head()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7872455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where 'Nombre de circulations prévues' is 0 and 'Nombre de trains annulés' > 0\n",
    "\n",
    "df_raw.drop(df_raw[(df_raw['Nombre de circulations prévues'] == 0) & (df_raw['Nombre de trains annulés'] > 0)].index, inplace=True)\n",
    "\n",
    "# checking now in the raw dataset if there are still rows where 'Nombre de circulations prévues' < 'Nombre de trains annulés'\n",
    "invalid_rows_final = df_raw[df_raw['Nombre de circulations prévues'] < df_raw['Nombre de trains annulés']]\n",
    "print(f\"Number of invalid rows after dropping inconsistent data: {len(invalid_rows_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54656fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[df_raw['Nombre de trains en retard au départ'] > df_raw[\"Nombre de circulations prévues\"]].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca99794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etude Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the rows where 'Retard moyen de tous les trains à l'arrivée' < -30, meaning the train has more than \n",
    "# 30 minutes of advance on the schedule, which might seem a little strange.\n",
    "\n",
    "df_raw[df_raw['Retard moyen de tous les trains à l\\'arrivée'] < -30].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad25e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet_optimized(csv_file_path, parquet_file_path, index_col=None, \n",
    "                             compression='snappy', chunk_size=None, separator=','):\n",
    "    \"\"\"\n",
    "    Converts a CSV file to Parquet format with optimizations using PyArrow.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): Path to the input CSV file.\n",
    "        parquet_file_path (str): Path where the output Parquet file will be saved.\n",
    "        index_col (str, optional): Name of the column to use as index (None by default).\n",
    "        compression (str, optional): Compression algorithm to use ('snappy', 'gzip', 'brotli', 'zstd').\n",
    "        chunk_size (int, optional): Number of rows to read at a time for large files (None reads all at once).\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if conversion succeeded, False otherwise.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Starting CSV file reading: {csv_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV with Pandas\n",
    "        # Using low_memory=False to prevent dtype warnings on large files\n",
    "        df = pd.read_csv(csv_file_path, index_col=index_col, low_memory=False, chunksize=chunk_size, sep=separator)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at specified location: {csv_file_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return False\n",
    "\n",
    "    read_time = time.time()\n",
    "    print(f\"CSV reading completed in {read_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Rows read: {len(df):,}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Schema Optimization\n",
    "    # Convert DataFrame to PyArrow Table\n",
    "    # This step infers PyArrow schema from Pandas types\n",
    "    print(\"Converting to PyArrow table...\")\n",
    "    try:\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting to PyArrow table: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Add Custom Metadata\n",
    "    # Metadata is stored in the Parquet file footer\n",
    "    metadata = {\n",
    "        'creation_tool': 'csv_to_parquet_optimized.py',\n",
    "        'conversion_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'source_file': csv_file_path,\n",
    "        'original_row_count': str(len(df)),\n",
    "        'original_column_count': str(len(df.columns)),\n",
    "        'compression_algorithm': compression\n",
    "    }\n",
    "    \n",
    "    # Integrate metadata into schema\n",
    "    # PyArrow stores metadata at the schema level\n",
    "    existing_metadata = table.schema.metadata or {}\n",
    "    existing_metadata[b'custom_metadata'] = str(metadata).encode('utf8')\n",
    "    table = table.replace_schema_metadata(existing_metadata)\n",
    "    \n",
    "    print(f\"Writing Parquet file with '{compression}' compression...\")\n",
    "\n",
    "    # Write Parquet file\n",
    "    # PyArrow provides efficient Parquet writing\n",
    "    try:\n",
    "        pq.write_table(\n",
    "            table, \n",
    "            parquet_file_path, \n",
    "            compression=compression,\n",
    "            use_dictionary=True,       # Efficient for categorical columns\n",
    "            write_statistics=True,     # Enable statistics for better query performance\n",
    "            row_group_size=100000,     # Optimize row group size for balance between memory and I/O\n",
    "            version='2.6'              # Use newer Parquet format version for better features\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing Parquet file: {e}\")\n",
    "        return False\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Display results\n",
    "    import os\n",
    "    csv_size = os.path.getsize(csv_file_path) / 1024**2\n",
    "    parquet_size = os.path.getsize(parquet_file_path) / 1024**2\n",
    "    compression_ratio = (1 - parquet_size / csv_size) * 100\n",
    "    \n",
    "    print(f\"\\nConversion successful!\")\n",
    "    print(f\"Parquet file saved to: {parquet_file_path}\")\n",
    "    print(f\"Original CSV size: {csv_size:.2f} MB\")\n",
    "    print(f\"Parquet file size: {parquet_size:.2f} MB\")\n",
    "    print(f\"Compression ratio: {compression_ratio:.1f}%\")\n",
    "    print(f\"Total duration: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10165cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_parquet_file(parquet_file_path, num_rows_preview=5):\n",
    "    \"\"\"\n",
    "    Verifies and displays information about a Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        parquet_file_path (str): Path to the Parquet file to verify.\n",
    "        num_rows_preview (int): Number of rows to preview (default: 5).\n",
    "    \"\"\"\n",
    "    print(f\"\\nVerifying Parquet file: {parquet_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(parquet_file_path)\n",
    "        \n",
    "        print(f\"\\nParquet Schema:\")\n",
    "        print(parquet_file.schema)\n",
    "        \n",
    "        print(f\"\\nFile metadata:\")\n",
    "        print(f\"Number of row groups: {parquet_file.num_row_groups}\")\n",
    "        print(f\"Total rows: {parquet_file.metadata.num_rows:,}\")\n",
    "        \n",
    "        # Read custom metadata\n",
    "        metadata_bytes = parquet_file.metadata.metadata.get(b'custom_metadata')\n",
    "        if metadata_bytes:\n",
    "            print(f\"\\nCustom metadata:\")\n",
    "            print(metadata_bytes.decode('utf8'))\n",
    "        \n",
    "        # Sample first few rows using pandas read_parquet\n",
    "        print(f\"\\nFirst {num_rows_preview} rows preview:\")\n",
    "        df_sample = pd.read_parquet(parquet_file_path, engine='pyarrow').head(num_rows_preview)\n",
    "        display(df_sample)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying Parquet file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1eaa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual file paths\n",
    "input_csv = 'data_clean.csv'\n",
    "output_parquet = 'data.parquet'\n",
    "    \n",
    " # Convert CSV to Parquet\n",
    "success = csv_to_parquet_optimized(\n",
    "    input_csv, \n",
    "    output_parquet, \n",
    "    compression='snappy', # Options: 'snappy', 'gzip', 'brotli', 'zstd'\n",
    "    separator=\";\"\n",
    ")\n",
    "    \n",
    "# Verify the conversion if successful\n",
    "if success:\n",
    "    verify_parquet_file(output_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86b1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10687,)]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "dbms_columnar = duckdb.connect(\"database.duckdb\")\n",
    "\n",
    "dbms_columnar.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS TGV_table AS\n",
    "    SELECT * FROM 'data.parquet';\n",
    "\"\"\")\n",
    "\n",
    "print(dbms_columnar.execute(\"SELECT COUNT(*) FROM TGV_table\").fetchall())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
